{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f41a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HMC with dual averaging and trajectory length 0.20...\n",
      "0.25\n",
      "0.125\n",
      "find_reasonable_epsilon= 0.125\n",
      "Done. Final epsilon = 0.300847.\n",
      "(M+Madapt) / Functions called: 0.569866\n",
      "Percentiles\n",
      "[[-0.96223025 -1.92113153]\n",
      " [ 0.04036767  0.08803081]\n",
      " [ 1.00990273  2.01081402]]\n",
      "Mean\n",
      "[0.02628615 0.0528987 ]\n",
      "Stddev\n",
      "[1.01110121 2.01830967]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "\"\"\"\n",
    "This package implements the No-U-Turn Sampler (NUTS) algorithm 6 from the NUTS\n",
    "paper (Hoffman & Gelman, 2011).\n",
    "Content\n",
    "-------\n",
    "The package mainly contains:\n",
    "  nuts6                     return samples using the NUTS\n",
    "  test_nuts6                example usage of this package\n",
    "and subroutines of nuts6:\n",
    "  build_tree                the main recursion in NUTS\n",
    "  find_reasonable_epsilon   Heuristic for choosing an initial value of epsilon\n",
    "  leapfrog                  Perfom a leapfrog jump in the Hamiltonian space\n",
    "  stop_criterion            Compute the stop condition in the main loop\n",
    "A few words about NUTS\n",
    "----------------------\n",
    "Hamiltonian Monte Carlo or Hybrid Monte Carlo (HMC) is a Markov chain Monte\n",
    "Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to\n",
    "correlated parameters, biggest weakness of many MCMC methods. Instead, it takes\n",
    "a series of steps informed by first-order gradient information.\n",
    "This feature allows it to converge much more quickly to high-dimensional target\n",
    "distributions compared to simpler methods such as Metropolis, Gibbs sampling\n",
    "(and derivatives).\n",
    "However, HMC's performance is highly sensitive to two user-specified\n",
    "parameters: a step size, and a desired number of steps.  In particular, if the\n",
    "number of steps is too small then the algorithm will just exhibit random walk\n",
    "behavior, whereas if it is too large it will waste computations.\n",
    "Hoffman & Gelman introduced NUTS or the No-U-Turn Sampler, an extension to HMC\n",
    "that eliminates the need to set a number of steps.  NUTS uses a recursive\n",
    "algorithm to find likely candidate points that automatically stops when it\n",
    "starts to double back and retrace its steps.  Empirically, NUTS perform at\n",
    "least as effciently as and sometimes more effciently than a well tuned standard\n",
    "HMC method, without requiring user intervention or costly tuning runs.\n",
    "Moreover, Hoffman & Gelman derived a method for adapting the step size\n",
    "parameter on the fly based on primal-dual averaging.  NUTS can thus be used\n",
    "with no hand-tuning at all.\n",
    "In practice, the implementation still requires a number of steps, a burning\n",
    "period and a stepsize. However, the stepsize will be optimized during the\n",
    "burning period, and the final values of all the user-defined values will be\n",
    "revised by the algorithm.\n",
    "reference: arXiv:1111.4246\n",
    "\"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte\n",
    "Carlo\", Matthew D. Hoffman & Andrew Gelman\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy import log, exp, sqrt\n",
    "#from .helpers import progress_range\n",
    "\n",
    "__all__ = ['nuts6']\n",
    "\n",
    "def leapfrog(theta, r, grad, epsilon, f):\n",
    "    \"\"\" Perfom a leapfrog jump in the Hamiltonian space\n",
    "    INPUTS\n",
    "    ------\n",
    "    theta: ndarray[float, ndim=1]\n",
    "        initial parameter position\n",
    "    r: ndarray[float, ndim=1]\n",
    "        initial momentum\n",
    "    grad: float\n",
    "        initial gradient value\n",
    "    epsilon: float\n",
    "        step size\n",
    "    f: callable\n",
    "        it should return the log probability and gradient evaluated at theta\n",
    "        logp, grad = f(theta)\n",
    "    OUTPUTS\n",
    "    -------\n",
    "    thetaprime: ndarray[float, ndim=1]\n",
    "        new parameter position\n",
    "    rprime: ndarray[float, ndim=1]\n",
    "        new momentum\n",
    "    gradprime: float\n",
    "        new gradient\n",
    "    logpprime: float\n",
    "        new lnp\n",
    "    \"\"\"\n",
    "    # make half step in r\n",
    "    rprime = r + 0.5 * epsilon * grad\n",
    "    # make new step in theta\n",
    "    thetaprime = theta + epsilon * rprime\n",
    "    #compute new gradient\n",
    "    logpprime, gradprime = f(thetaprime)\n",
    "    # make half step in r again\n",
    "    rprime = rprime + 0.5 * epsilon * gradprime\n",
    "    return thetaprime, rprime, gradprime, logpprime\n",
    "\n",
    "\n",
    "def find_reasonable_epsilon(theta0, grad0, logp0, f):\n",
    "    \"\"\" Heuristic for choosing an initial value of epsilon \"\"\"\n",
    "    epsilon = 1.\n",
    "    r0 = np.random.normal(0., 1., len(theta0))\n",
    "\n",
    "    # Figure out what direction we should be moving epsilon.\n",
    "    _, rprime, gradprime, logpprime = leapfrog(theta0, r0, grad0, epsilon, f)\n",
    "    # brutal! This trick make sure the step is not huge leading to infinite\n",
    "    # values of the likelihood. This could also help to make sure theta stays\n",
    "    # within the prior domain (if any)\n",
    "    k = 1.\n",
    "    while np.isinf(logpprime) or np.isinf(gradprime).any():\n",
    "        k *= 0.5\n",
    "        _, rprime, _, logpprime = leapfrog(theta0, r0, grad0, epsilon * k, f)\n",
    "\n",
    "    epsilon = 0.5 * k * epsilon\n",
    "\n",
    "    # acceptprob = np.exp(logpprime - logp0 - 0.5 * (np.dot(rprime, rprime.T) - np.dot(r0, r0.T)))\n",
    "    # a = 2. * float((acceptprob > 0.5)) - 1.\n",
    "    logacceptprob = logpprime-logp0-0.5*(np.dot(rprime, rprime)-np.dot(r0,r0))\n",
    "    a = 1. if logacceptprob > np.log(0.5) else -1.\n",
    "    # Keep moving epsilon in that direction until acceptprob crosses 0.5.\n",
    "    # while ( (acceptprob ** a) > (2. ** (-a))):\n",
    "    while a * logacceptprob > -a * np.log(2):\n",
    "        epsilon = epsilon * (2. ** a)\n",
    "        print(epsilon)\n",
    "        _, rprime, _, logpprime = leapfrog(theta0, r0, grad0, epsilon, f)\n",
    "        # acceptprob = np.exp(logpprime - logp0 - 0.5 * ( np.dot(rprime, rprime.T) - np.dot(r0, r0.T)))\n",
    "        logacceptprob = logpprime-logp0-0.5*(np.dot(rprime, rprime)-np.dot(r0,r0))\n",
    "\n",
    "    print(\"find_reasonable_epsilon=\", epsilon)\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "def stop_criterion(thetaminus, thetaplus, rminus, rplus):\n",
    "    \"\"\" Compute the stop condition in the main loop\n",
    "    dot(dtheta, rminus) >= 0 & dot(dtheta, rplus >= 0)\n",
    "    INPUTS\n",
    "    ------\n",
    "    thetaminus, thetaplus: ndarray[float, ndim=1]\n",
    "        under and above position\n",
    "    rminus, rplus: ndarray[float, ndim=1]\n",
    "        under and above momentum\n",
    "    OUTPUTS\n",
    "    -------\n",
    "    criterion: bool\n",
    "        return if the condition is valid\n",
    "    \"\"\"\n",
    "    dtheta = thetaplus - thetaminus\n",
    "    return (np.dot(dtheta, rminus.T) >= 0) & (np.dot(dtheta, rplus.T) >= 0)\n",
    "\n",
    "\n",
    "def build_tree(theta, r, grad, logu, v, j, epsilon, f, joint0):\n",
    "    \"\"\"The main recursion.\"\"\"\n",
    "    if (j == 0):\n",
    "        # Base case: Take a single leapfrog step in the direction v.\n",
    "        thetaprime, rprime, gradprime, logpprime = leapfrog(theta, r, grad, v * epsilon, f)\n",
    "        joint = logpprime - 0.5 * np.dot(rprime, rprime.T)\n",
    "        # Is the new point in the slice?\n",
    "        nprime = int(logu < joint)\n",
    "        # Is the simulation wildly inaccurate?\n",
    "        sprime = int((logu - 1000.) < joint)\n",
    "        # Set the return values---minus=plus for all things here, since the\n",
    "        # \"tree\" is of depth 0.\n",
    "        thetaminus = thetaprime[:]\n",
    "        thetaplus = thetaprime[:]\n",
    "        rminus = rprime[:]\n",
    "        rplus = rprime[:]\n",
    "        gradminus = gradprime[:]\n",
    "        gradplus = gradprime[:]\n",
    "        # Compute the acceptance probability.\n",
    "        alphaprime = min(1., np.exp(joint - joint0))\n",
    "        #alphaprime = min(1., np.exp(logpprime - 0.5 * np.dot(rprime, rprime.T) - joint0))\n",
    "        nalphaprime = 1\n",
    "    else:\n",
    "        # Recursion: Implicitly build the height j-1 left and right subtrees.\n",
    "        thetaminus, rminus, gradminus, thetaplus, rplus, gradplus, thetaprime, gradprime, logpprime, nprime, sprime, alphaprime, nalphaprime = build_tree(theta, r, grad, logu, v, j - 1, epsilon, f, joint0)\n",
    "        # No need to keep going if the stopping criteria were met in the first subtree.\n",
    "        if (sprime == 1):\n",
    "            if (v == -1):\n",
    "                thetaminus, rminus, gradminus, _, _, _, thetaprime2, gradprime2, logpprime2, nprime2, sprime2, alphaprime2, nalphaprime2 = build_tree(thetaminus, rminus, gradminus, logu, v, j - 1, epsilon, f, joint0)\n",
    "            else:\n",
    "                _, _, _, thetaplus, rplus, gradplus, thetaprime2, gradprime2, logpprime2, nprime2, sprime2, alphaprime2, nalphaprime2 = build_tree(thetaplus, rplus, gradplus, logu, v, j - 1, epsilon, f, joint0)\n",
    "            # Choose which subtree to propagate a sample up from.\n",
    "            if (np.random.uniform() < (float(nprime2) / max(float(int(nprime) + int(nprime2)), 1.))):\n",
    "                thetaprime = thetaprime2[:]\n",
    "                gradprime = gradprime2[:]\n",
    "                logpprime = logpprime2\n",
    "            # Update the number of valid points.\n",
    "            nprime = int(nprime) + int(nprime2)\n",
    "            # Update the stopping criterion.\n",
    "            sprime = int(sprime and sprime2 and stop_criterion(thetaminus, thetaplus, rminus, rplus))\n",
    "            # Update the acceptance probability statistics.\n",
    "            alphaprime = alphaprime + alphaprime2\n",
    "            nalphaprime = nalphaprime + nalphaprime2\n",
    "\n",
    "    return thetaminus, rminus, gradminus, thetaplus, rplus, gradplus, thetaprime, gradprime, logpprime, nprime, sprime, alphaprime, nalphaprime\n",
    "\n",
    "\n",
    "def nuts6(f, M, Madapt, theta0, delta=0.6, progress=False):\n",
    "    \"\"\"\n",
    "    Implements the No-U-Turn Sampler (NUTS) algorithm 6 from from the NUTS\n",
    "    paper (Hoffman & Gelman, 2011).\n",
    "    Runs Madapt steps of burn-in, during which it adapts the step size\n",
    "    parameter epsilon, then starts generating samples to return.\n",
    "    Note the initial step size is tricky and not exactly the one from the\n",
    "    initial paper.  In fact the initial step size could be given by the user in\n",
    "    order to avoid potential problems\n",
    "    INPUTS\n",
    "    ------\n",
    "    epsilon: float\n",
    "        step size\n",
    "        see nuts8 if you want to avoid tuning this parameter\n",
    "    f: callable\n",
    "        it should return the log probability and gradient evaluated at theta\n",
    "        logp, grad = f(theta)\n",
    "    M: int\n",
    "        number of samples to generate.\n",
    "    Madapt: int\n",
    "        the number of steps of burn-in/how long to run the dual averaging\n",
    "        algorithm to fit the step size epsilon.\n",
    "    theta0: ndarray[float, ndim=1]\n",
    "        initial guess of the parameters.\n",
    "    KEYWORDS\n",
    "    --------\n",
    "    delta: float\n",
    "        targeted acceptance fraction\n",
    "    progress: bool\n",
    "        whether to show progress (requires tqdm module for full functionality)\n",
    "    OUTPUTS\n",
    "    -------\n",
    "    samples: ndarray[float, ndim=2]\n",
    "    M x D matrix of samples generated by NUTS.\n",
    "    note: samples[0, :] = theta0\n",
    "    \"\"\"\n",
    "\n",
    "    if len(np.shape(theta0)) > 1:\n",
    "        raise ValueError('theta0 is expected to be a 1-D array')\n",
    "\n",
    "    D = len(theta0)\n",
    "    samples = np.empty((M + Madapt, D), dtype=float)\n",
    "    lnprob = np.empty(M + Madapt, dtype=float)\n",
    "\n",
    "    logp, grad = f(theta0)\n",
    "    samples[0, :] = theta0\n",
    "    lnprob[0] = logp\n",
    "\n",
    "    # Choose a reasonable first epsilon by a simple heuristic.\n",
    "    epsilon = find_reasonable_epsilon(theta0, grad, logp, f)\n",
    "\n",
    "    # Parameters to the dual averaging algorithm.\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    kappa = 0.75\n",
    "    mu = log(10. * epsilon)\n",
    "\n",
    "    # Initialize dual averaging algorithm.\n",
    "    epsilonbar = 1\n",
    "    Hbar = 0\n",
    "\n",
    "    for m in range(1, M + Madapt):\n",
    "        # Resample momenta.\n",
    "        r0 = np.random.normal(0, 1, D)\n",
    "\n",
    "        #joint lnp of theta and momentum r\n",
    "        joint = logp - 0.5 * np.dot(r0, r0.T)\n",
    "\n",
    "        # Resample u ~ uniform([0, exp(joint)]).\n",
    "        # Equivalent to (log(u) - joint) ~ exponential(1).\n",
    "        logu = float(joint - np.random.exponential(1, size=1))\n",
    "\n",
    "        # if all fails, the next sample will be the previous one\n",
    "        samples[m, :] = samples[m - 1, :]\n",
    "        lnprob[m] = lnprob[m - 1]\n",
    "\n",
    "        # initialize the tree\n",
    "        thetaminus = samples[m - 1, :]\n",
    "        thetaplus = samples[m - 1, :]\n",
    "        rminus = r0[:]\n",
    "        rplus = r0[:]\n",
    "        gradminus = grad[:]\n",
    "        gradplus = grad[:]\n",
    "\n",
    "        j = 0  # initial heigth j = 0\n",
    "        n = 1  # Initially the only valid point is the initial point.\n",
    "        s = 1  # Main loop: will keep going until s == 0.\n",
    "\n",
    "        while (s == 1):\n",
    "            # Choose a direction. -1 = backwards, 1 = forwards.\n",
    "            v = int(2 * (np.random.uniform() < 0.5) - 1)\n",
    "\n",
    "            # Double the size of the tree.\n",
    "            if (v == -1):\n",
    "                thetaminus, rminus, gradminus, _, _, _, thetaprime, gradprime, logpprime, nprime, sprime, alpha, nalpha = build_tree(thetaminus, rminus, gradminus, logu, v, j, epsilon, f, joint)\n",
    "            else:\n",
    "                _, _, _, thetaplus, rplus, gradplus, thetaprime, gradprime, logpprime, nprime, sprime, alpha, nalpha = build_tree(thetaplus, rplus, gradplus, logu, v, j, epsilon, f, joint)\n",
    "\n",
    "            # Use Metropolis-Hastings to decide whether or not to move to a\n",
    "            # point from the half-tree we just generated.\n",
    "            _tmp = min(1, float(nprime) / float(n))\n",
    "            if (sprime == 1) and (np.random.uniform() < _tmp):\n",
    "                samples[m, :] = thetaprime[:]\n",
    "                lnprob[m] = logpprime\n",
    "                logp = logpprime\n",
    "                grad = gradprime[:]\n",
    "            # Update number of valid points we've seen.\n",
    "            n += nprime\n",
    "            # Decide if it's time to stop.\n",
    "            s = sprime and stop_criterion(thetaminus, thetaplus, rminus, rplus)\n",
    "            # Increment depth.\n",
    "            j += 1\n",
    "\n",
    "        # Do adaptation of epsilon if we're still doing burn-in.\n",
    "        eta = 1. / float(m + t0)\n",
    "        Hbar = (1. - eta) * Hbar + eta * (delta - alpha / float(nalpha))\n",
    "        if (m <= Madapt):\n",
    "            epsilon = exp(mu - sqrt(m) / gamma * Hbar)\n",
    "            eta = m ** -kappa\n",
    "            epsilonbar = exp((1. - eta) * log(epsilonbar) + eta * log(epsilon))\n",
    "        else:\n",
    "            epsilon = epsilonbar\n",
    "    samples = samples[Madapt:, :]\n",
    "    lnprob = lnprob[Madapt:]\n",
    "    return samples, lnprob, epsilon\n",
    "\n",
    "\n",
    "def test_nuts6():\n",
    "    \"\"\" Example usage of nuts6: sampling a 2d highly correlated Gaussian distribution \"\"\"\n",
    "\n",
    "    class Counter:\n",
    "        def __init__(self, c=0):\n",
    "            self.c = c\n",
    "\n",
    "    c = Counter()\n",
    "    def correlated_normal(theta):\n",
    "        \"\"\"\n",
    "        Example of a target distribution that could be sampled from using NUTS.\n",
    "        (Although of course you could sample from it more efficiently)\n",
    "        Doesn't include the normalizing constant.\n",
    "        \"\"\"\n",
    "\n",
    "        # Precision matrix with covariance [1, 1.98; 1.98, 4].\n",
    "        # A = np.linalg.inv( cov )\n",
    "        A = np.asarray([[50.251256, -24.874372],[-24.874372, 12.562814]])\n",
    "        #A = np.eye(2)\n",
    "\n",
    "        # add the counter to count how many times this function is called\n",
    "        c.c += 1\n",
    "\n",
    "        grad = -np.dot(theta, A)\n",
    "        logp = 0.5 * np.dot(grad, theta.T)\n",
    "        return logp, grad\n",
    "    \n",
    "    def correlated_normal(p):\n",
    "        inv_cov = np.asarray([[50.251256, -24.874372],[-24.874372, 12.562814]])\n",
    "        c.c += 1\n",
    "        grad = -p @ inv_cov\n",
    "        logP = 0.5 * grad @ p.T\n",
    "        return logP, grad\n",
    "\n",
    "    D = 2\n",
    "    M = 100000\n",
    "    Madapt = 5000\n",
    "    theta0 = np.random.normal(0, 1, D)\n",
    "    delta = 0.2\n",
    "\n",
    "    mean = np.zeros(2)\n",
    "    cov = np.asarray([[1, 1.98],\n",
    "                      [1.98, 4]])\n",
    "    #cov = np.eye(2)\n",
    "\n",
    "    print('Running HMC with dual averaging and trajectory length %0.2f...' % delta)\n",
    "    samples, lnprob, epsilon = nuts6(correlated_normal, M, Madapt, theta0, delta)\n",
    "    print('Done. Final epsilon = %f.' % epsilon)\n",
    "    print('(M+Madapt) / Functions called: %f' % ((M+Madapt)/float(c.c)))\n",
    "\n",
    "    samples = samples[1::10, :]\n",
    "    print('Percentiles')\n",
    "    print (np.percentile(samples, [16, 50, 84], axis=0))\n",
    "    print('Mean')\n",
    "    print (np.mean(samples, axis=0))\n",
    "    print('Stddev')\n",
    "    print (np.std(samples, axis=0))\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        import pylab as plt\n",
    "    temp = np.random.multivariate_normal(mean, cov, size=500)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(temp[:, 0], temp[:, 1], '.')\n",
    "    plt.plot(samples[:, 0], samples[:, 1], 'r+')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.hist(samples[:,0], bins=50)\n",
    "    plt.xlabel(\"x-samples\")\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.hist(samples[:,1], bins=50)\n",
    "    plt.xlabel(\"y-samples\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_nuts6()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
